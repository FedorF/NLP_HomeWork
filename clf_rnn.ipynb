{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fomichev 292\n",
    "### мне удалось собрать и обучить нейронку, но возникла путаница с версией питона и почти везде вылетают ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning for Natural Language Processing\n",
    "\n",
    "\n",
    " * Simple text representations, bag of words\n",
    " * Word embedding and... not just another word2vec this time\n",
    " * rnn for text\n",
    " * Aggregating several data sources \"the hard way\"\n",
    " * Solving ~somewhat~ real ML problem with ~almost~ end-to-end deep learning\n",
    " \n",
    "\n",
    "Special thanks to Irina Golzmann for help with technical part, task prepared by Александр Панин, jheuristic@yandex-team.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "You will require nltk v3.2 to solve this assignment\n",
    "\n",
    "__It is really important that the version is 3.2, otherwize russian tokenizer might not work__\n",
    "\n",
    "Install/update\n",
    "* `sudo pip install --upgrade nltk==3.2`\n",
    "* If you don't remember when was the last pip upgrade, `sudo pip install --upgrade pip`\n",
    "\n",
    "If for some reason you can't or won't switch to nltk v3.2, just make sure that russian words are tokenized properly with RegeExpTokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For students with low-RAM machines\n",
    " * This assignment can be accomplished with even the low-tier hardware (<= 4Gb RAM) \n",
    " * If that is the case, turn flag \"low_RAM_mode\" below to True\n",
    " * If you have around 8GB memory, it is unlikely that you will feel constrained by memory.\n",
    " * In case you are using a PC from last millenia, consider setting very_low_RAM=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low_RAM_mode = True\n",
    "very_low_RAM = False  #If you have <3GB RAM, set BOTH to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fed/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:280: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Ex-kaggle-competition on prohibited content detection\n",
    "\n",
    "There goes the description - https://www.kaggle.com/c/avito-prohibited-content\n",
    "\n",
    "\n",
    "### Download\n",
    "High-RAM mode,\n",
    " * Download avito_train.tsv from competition data files\n",
    "Low-RAM-mode,\n",
    " * Download downsampled dataset from here\n",
    "     * archive https://yadi.sk/d/l0p4lameqw3W8\n",
    "     * raw https://yadi.sk/d/I1v7mZ6Sqw2WK (in case you feel masochistic)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# What's inside\n",
    "Different kinds of features:\n",
    "* 2 text fields - title and description\n",
    "* Special features - price, number of e-mails, phones, etc\n",
    "* Category and subcategory - unsurprisingly, categorical features\n",
    "* Attributes - more factors\n",
    "\n",
    "Only 1 binary target whether or not such advertisement contains prohibited materials\n",
    "* criminal, misleading, human reproduction-related, etc\n",
    "* diving into the data may result in prolonged sleep disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not low_RAM_mode:\n",
    "    # a lot of ram\n",
    "    df = pd.read_csv(\"avito_train.tsv\",sep='\\t')\n",
    "else:\n",
    "    #aroung 4GB ram\n",
    "    df = pd.read_csv(\"avito_train_1kk.tsv\",sep='\\t')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1204949, 13) 0.228222107326\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000010</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Toyota Sera, 1991</td>\n",
       "      <td>Новая оригинальная линзованая оптика на ксенон...</td>\n",
       "      <td>{\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000094</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>Костюм Steilmann</td>\n",
       "      <td>Юбка и топ из панбархата. Под топ  трикотажная...</td>\n",
       "      <td>{\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000299</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>Костюм Didriksons Boardman, размер 100, краги,...</td>\n",
       "      <td>Костюм Didriksons Boardman, в отличном состоян...</td>\n",
       "      <td>{\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000309</td>\n",
       "      <td>Недвижимость</td>\n",
       "      <td>Квартиры</td>\n",
       "      <td>1-к квартира, 44 м², 9/20 эт.</td>\n",
       "      <td>В кирпичном пан.-м доме, продается одноком.-ая...</td>\n",
       "      <td>{\"Тип объявления\":\"Продам\", \"Количество комнат...</td>\n",
       "      <td>2642020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000317</td>\n",
       "      <td>Услуги</td>\n",
       "      <td>Предложения услуг</td>\n",
       "      <td>Поездки на таможню, печать в паспорте</td>\n",
       "      <td>Поездки на таможню гражданам СНГ для пересечен...</td>\n",
       "      <td>{\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     itemid      category                subcategory  \\\n",
       "0  10000010     Транспорт      Автомобили с пробегом   \n",
       "1  10000094   Личные вещи  Одежда, обувь, аксессуары   \n",
       "2  10000299   Личные вещи     Детская одежда и обувь   \n",
       "3  10000309  Недвижимость                   Квартиры   \n",
       "4  10000317        Услуги          Предложения услуг   \n",
       "\n",
       "                                               title  \\\n",
       "0                                  Toyota Sera, 1991   \n",
       "1                                   Костюм Steilmann   \n",
       "2  Костюм Didriksons Boardman, размер 100, краги,...   \n",
       "3                      1-к квартира, 44 м², 9/20 эт.   \n",
       "4              Поездки на таможню, печать в паспорте   \n",
       "\n",
       "                                         description  \\\n",
       "0  Новая оригинальная линзованая оптика на ксенон...   \n",
       "1  Юбка и топ из панбархата. Под топ  трикотажная...   \n",
       "2  Костюм Didriksons Boardman, в отличном состоян...   \n",
       "3  В кирпичном пан.-м доме, продается одноком.-ая...   \n",
       "4  Поездки на таможню гражданам СНГ для пересечен...   \n",
       "\n",
       "                                               attrs    price  is_proved  \\\n",
       "0  {\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...   150000        NaN   \n",
       "1  {\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...     1500        NaN   \n",
       "2  {\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...     3000        NaN   \n",
       "3  {\"Тип объявления\":\"Продам\", \"Количество комнат...  2642020        NaN   \n",
       "4  {\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...     1500        0.0   \n",
       "\n",
       "   is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "0           0           0           0         0         0.03  \n",
       "1           0           0           0         0         0.41  \n",
       "2           0           0           0         0         5.49  \n",
       "3           0           1           0         0        22.47  \n",
       "4           1           0           0         0         1.43  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print df.shape, df.is_blocked.mean()\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://kaggle2.blob.core.windows.net/competitions/kaggle/3929/media/Ad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio 0.228222107326\n",
      "Count: 1204949\n"
     ]
    }
   ],
   "source": [
    "print \"Blocked ratio\",df.is_blocked.mean()\n",
    "print \"Count:\",len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance-out the classes\n",
    "* Vast majority of data samples are non-prohibited\n",
    " * 250k banned out of 4kk\n",
    " * Let's just downsample random 250k legal samples to make further steps less computationally demanding\n",
    " * If you aim for high Kaggle score, consider a smarter approach to that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio: 0.5\n",
      "Count: 549992\n",
      "549992\n"
     ]
    }
   ],
   "source": [
    "#downsample\n",
    "\n",
    "\n",
    "# <downsample data so that both classes have approximately equal ratios>\n",
    "\n",
    "blocked = df[df.is_blocked == 1]\n",
    "df = df[df.is_blocked == 0].sample(274996)\n",
    "df = df.append(blocked)\n",
    "print \"Blocked ratio:\",df.is_blocked.mean()\n",
    "print \"Count:\",len(df)\n",
    "print len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "assert df.is_blocked.mean() < 0.51\n",
    "assert df.is_blocked.mean() > 0.49\n",
    "assert len(df) <= 560000\n",
    "\n",
    "print \"All tests passed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tokenizing\n",
    "\n",
    "First, we create a dictionary of all existing words.\n",
    "Assign each word a number - it's Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "#All texts\n",
    "all_texts = np.hstack([df.description.values,df.title.values])\n",
    "\n",
    "\n",
    "#Compute token frequencies\n",
    "for s in all_texts:\n",
    "    if type(s) is not str:\n",
    "        continue\n",
    "    s = s.decode('utf8').lower()\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    for token in tokens:\n",
    "        token_counts[token] +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rare tokens\n",
    "\n",
    "We are unlikely to make use of words that are only seen a few times throughout the corpora.\n",
    "\n",
    "Again, if you want to beat Kaggle competition metrics, consider doing something better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm0AAAEyCAYAAACoO+cBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDQBgAEIbAMAAhDYAgAEIbQAAA/j/QJXNbLMs2SUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f747ab54c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Word frequency distribution, just for kicks\n",
    "plt.figure(figsize=(10, 5))\n",
    "_=plt.hist(token_counts.values(),range=[0,50],bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Select only the tokens that had at least 10 occurences in the corpora.\n",
    "#Use token_counts.\n",
    "\n",
    "min_count = 10\n",
    "tokens = [token for token in token_counts.keys() if token_counts[token] > min_count]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {t:i+1 for i,t in enumerate(tokens)}\n",
    "null_token = \"NULL\"\n",
    "token_to_id[null_token] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tokens: 83071\n",
      "Alarm! Too many tokens. You might have messed up when pruning rare ones -- unless you know what you're doin' ofc\n"
     ]
    }
   ],
   "source": [
    "print \"# Tokens:\",len(token_to_id)\n",
    "if len(token_to_id) < 30000:\n",
    "    print \"Alarm! It seems like there are too few tokens. Make sure you updated NLTK and applied correct thresholds -- unless you now what you're doing, ofc\"\n",
    "if len(token_to_id) < 1000000:\n",
    "    print \"Alarm! Too many tokens. You might have messed up when pruning rare ones -- unless you know what you're doin' ofc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace words with IDs\n",
    "Set a maximum length for titles and descriptions.\n",
    " * If string is longer that that limit - crop it, if less - pad with zeros.\n",
    " * Thus we obtain a matrix of size [n_samples]x[max_length]\n",
    " * Element at i,j - is an identifier of word j within sample i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, max_len=150):\n",
    "    token_matrix = []\n",
    "    for s in strings:\n",
    "        if type(s) is not str:\n",
    "            token_matrix.append([0]*max_len)\n",
    "            continue\n",
    "        s = s.decode('utf8').lower()\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        token_ids = map(lambda token: token_to_id.get(token,0), tokens)[:max_len]\n",
    "        token_ids += [0]*(max_len - len(token_ids))\n",
    "        token_matrix.append(token_ids)\n",
    "\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desc_tokens = vectorize(df.description.values,token_to_id,max_len = 150)\n",
    "title_tokens = vectorize(df.title.values,token_to_id,max_len = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data format examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (549992, 15)\n",
      "Отдам в заботливые руки кошку -> [81408 75699 63185 31464 77534     0     0     0     0     0] ...\n",
      "Утепление внутренней части входной двери -> [39195 37891 55146  2082 57276     0     0     0     0     0] ...\n",
      "ВАЗ 2109, 2003 -> [38169 33234 30230     0     0     0     0     0     0     0] ...\n"
     ]
    }
   ],
   "source": [
    "print \"Размер матрицы:\",title_tokens.shape\n",
    "for title, tokens in zip(df.title.values[:3],title_tokens[:3]):\n",
    "    print title,'->', tokens[:10],'...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ As you can see, our preprocessing is somewhat crude. Let us see if that is enough for our network __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-sequences\n",
    "\n",
    "\n",
    "Some data features are not text samples. E.g. price, # urls, category, etc\n",
    "\n",
    "They require a separate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All numeric features\n",
    "df_numerical_features = df[[\"phones_cnt\",\"emails_cnt\",\"urls_cnt\",\"price\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#One-hot-encoded category and subcategory\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "categories = []\n",
    "data_cat_subcat = df[[\"category\",\"subcategory\"]].values\n",
    "\n",
    "categories = [{\"category\":category_name, \"subcategory\":subcategory_name} \n",
    "              for category_name, subcategory_name in data_cat_subcat]\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "cat_one_hot = vectorizer.fit_transform(categories)\n",
    "cat_one_hot = pd.DataFrame(cat_one_hot,columns=vectorizer.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_non_text = pd.merge(\n",
    "    df_numerical_features,cat_one_hot,on = np.arange(len(cat_one_hot))\n",
    ")\n",
    "del df_non_text[\"key_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Target variable - whether or not sample contains prohibited material\n",
    "target = df.is_blocked.values.astype('int32')\n",
    "#Preprocessed titles\n",
    "title_tokens = title_tokens.astype('int32')\n",
    "#Preprocessed tokens\n",
    "desc_tokens = desc_tokens.astype('int32')\n",
    "\n",
    "#Non-sequences\n",
    "df_non_text = df_non_text.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split into training and test set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Difficulty selector:\n",
    "#Easy: split randomlysklearn.model_selection.train_test_split\n",
    "#Medium: select test set items that have item_ids strictly above that of training set\n",
    "#Hard: do whatever you want, but score yourself using kaggle private leaderboard\n",
    "\n",
    "title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = train_test_split(title_tokens, desc_tokens, df_non_text, target, test_size=0.2, random_state=123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed data [optional]\n",
    "\n",
    "* The next tab can be used to stash all the essential data matrices and get rid of the rest of the data.\n",
    " * Highly recommended if you have less than 1.5GB RAM left\n",
    "* To do that, you need to first run it with save_prepared_data=True, then restart the notebook and only run this tab with read_prepared_data=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save_prepared_data = True #save\n",
    "# read_prepared_data = False #load\n",
    "\n",
    "# #but not both at once\n",
    "# assert not (save_prepared_data and read_prepared_data)\n",
    "\n",
    "# if save_prepared_data:\n",
    "#     print \"Saving preprocessed data (may take up to 3 minutes)\"\n",
    "\n",
    "#     import pickle\n",
    "#     with open(\"preprocessed_data.pcl\",'w') as fout:\n",
    "#         pickle.dump(data_tuple,fout)\n",
    "#     with open(\"token_to_id.pcl\",'w') as fout:\n",
    "#         pickle.dump(token_to_id,fout)\n",
    "\n",
    "#     print \"готово\"\n",
    "    \n",
    "# elif read_prepared_data:\n",
    "#     print \"Reading saved data...\"\n",
    "    \n",
    "#     import pickle\n",
    "    \n",
    "#     with open(\"preprocessed_data.pcl\",'r') as fin:\n",
    "#         data_tuple = pickle.load(fin)\n",
    "#     title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple\n",
    "#     with open(\"token_to_id.pcl\",'r') as fin:\n",
    "#         token_to_id = pickle.load(fin)\n",
    "        \n",
    "#     #Re-importing libraries to allow staring noteboook from here\n",
    "#     import pandas as pd\n",
    "#     import numpy as np\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     %matplotlib inline\n",
    "   \n",
    "#     print \"done\"        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the monster\n",
    "\n",
    "Since we have several data sources, our neural network may differ from what you used to work with.\n",
    "\n",
    "* Separate input for titles: RNN\n",
    "* Separate input for description: RNN\n",
    "* Separate input for categorical features: обычные полносвязные слои или какие-нибудь трюки\n",
    " \n",
    "These three inputs must be blended somehow - concatenated or added.\n",
    "\n",
    "* Output: a simple binary classification\n",
    " * 1 sigmoidal with binary_crossentropy\n",
    " * 2 softmax with categorical_crossentropy - essentially the same as previous one\n",
    " * 1 neuron without nonlinearity (lambda x: x) +  hinge loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#libraries\n",
    "import theano\n",
    "import lasagne\n",
    "from theano import tensor as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3 inputs and a refere output\n",
    "title_token_ids = T.matrix(\"title_token_ids\",dtype='int32')\n",
    "desc_token_ids = T.matrix(\"desc_token_ids\",dtype='int32')\n",
    "categories = T.matrix(\"categories\",dtype='float32')\n",
    "target_y = T.ivector(\"is_blocked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_inp = lasagne.layers.InputLayer((None,title_tr.shape[1]),input_var=title_token_ids)\n",
    "descr_inp = lasagne.layers.InputLayer((None,desc_tr.shape[1]),input_var=desc_token_ids)\n",
    "cat_inp = lasagne.layers.InputLayer((None,nontext_tr.shape[1]), input_var=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Descriptions\n",
    "\n",
    "n_size = 64\n",
    "\n",
    "#word-wise embedding. We recommend to start from some 64 and improving after you are certain it works.\n",
    "descr_nn = lasagne.layers.EmbeddingLayer(descr_inp, input_size=len(token_to_id)+1, output_size=n_size)\n",
    "#descr_nn = RNN or LSTM over embedding, maybe several ones in a stack\n",
    "descr_nn = lasagne.layers.LSTMLayer(descr_nn, num_units=n_size)\n",
    "# Titles\n",
    "#title_nn = <Process titles somehow (title_inp)>\n",
    "title_nn = lasagne.layers.EmbeddingLayer(title_inp, input_size=len(token_to_id)+1, output_size=n_size)\n",
    "title_nn = lasagne.layers.LSTMLayer(title_nn, num_units=n_size)\n",
    "\n",
    "# Non-sequences\n",
    "#cat_nn = <Process non-sequences(cat_inp)>\n",
    "cat_nn = lasagne.layers.DenseLayer(cat_inp, num_units=n_size)\n",
    "cat_nn = lasagne.layers.ReshapeLayer(cat_nn, shape=(-1, 1, n_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nn = <merge three layers into one (e.g. lasagne.layers.concat) >                      \n",
    "nn = lasagne.layers.concat([descr_nn, title_nn])#, cat_nn])\n",
    "\n",
    "nn = lasagne.layers.DenseLayer(nn, 113) #your_lucky_number)\n",
    "nn = lasagne.layers.DropoutLayer(nn, p=0.3) #maybe_use_me)\n",
    "nn = lasagne.layers.DenseLayer(nn, 1, nonlinearity=lasagne.nonlinearities.linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "* The standard way:\n",
    " * prediction\n",
    " * loss\n",
    " * updates\n",
    " * training and evaluation functions\n",
    " \n",
    " \n",
    "* Hinge loss\n",
    " * $ L_i = \\max(0, \\delta - t_i p_i) $\n",
    " * delta is a tunable parameter: how far should a neuron be in the positive margin area for us to stop bothering about it\n",
    " * Function description may mention some +-1  limitations - this is not neccessary, at least as long as hinge loss has a __default__ flag `binary = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All trainable params\n",
    "weights = lasagne.layers.get_all_params(nn,trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Simple NN prediction\n",
    "prediction = lasagne.layers.get_output(nn)[:,0]\n",
    "\n",
    "#Hinge loss\n",
    "loss = lasagne.objectives.binary_hinge_loss(prediction,target_y,delta = 1, log_odds=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Weight optimization step\n",
    "#updates = <your favorite optimizer>\n",
    "updates = lasagne.updates.rmsprop(loss, weights, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinitic prediction \n",
    " * In case we use stochastic elements, e.g. dropout or noize\n",
    " * Compile a separate set of functions with deterministic prediction (deterministic = True)\n",
    " * Unless you think there's no neet for dropout there ofc. Btw is there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#deterministic version\n",
    "det_prediction = lasagne.layers.get_output(nn,deterministic=True)[:,0]\n",
    "\n",
    "#equivalent loss function\n",
    "det_loss = lasagne.objectives.binary_hinge_loss(prediction, target_y, delta = 1, log_odds=True).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coffee-lation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fed/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:1: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 2 is not part of the computational graph needed to compute the outputs: categories.\n",
      "To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.\n",
      "  if __name__ == '__main__':\n",
      "/home/fed/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:2: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 2 is not part of the computational graph needed to compute the outputs: categories.\n",
      "To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "train_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[loss,prediction],updates = updates, on_unused_input='warn', mode='FAST_RUN')\n",
    "eval_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[det_loss,det_prediction], on_unused_input='warn', mode='FAST_RUN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "* The regular way with loops over minibatches\n",
    "* Since the dataset is huge, we define epoch as some fixed amount of samples isntead of all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#average precision at K\n",
    "from oracle import APatK, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Out good old minibatch iterator now supports arbitrary amount of arrays (X,y,z)\n",
    "\n",
    "def iterate_minibatches(*arrays,**kwargs):\n",
    "    batchsize=kwargs.get(\"batchsize\",100)\n",
    "    shuffle = kwargs.get(\"shuffle\",True)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(arrays[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(arrays[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [arr[excerpt] for arr in arrays]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking guide\n",
    "\n",
    "* batch_size - how many samples are processed per function call\n",
    "  * optimization gets slower, but more stable, as you increase it.\n",
    "  * May consider increasing it halfway through training\n",
    "* minibatches_per_epoch - max amount of minibatches per epoch\n",
    "  * Does not affect training. Lesser value means more frequent and less stable printing\n",
    "  * Setting it to less than 10 is only meaningfull if you want to make sure your NN does not break down after one epoch\n",
    "* n_epochs - total amount of epochs to train for\n",
    "  * `n_epochs = 10**10` and manual interrupting is still an option\n",
    "\n",
    "\n",
    "Tips:\n",
    "\n",
    "* With small minibatches_per_epoch, network quality may jump around 0.5 for several epochs\n",
    "\n",
    "* AUC is the most stable of all three metrics\n",
    "\n",
    "* Average Precision at top 2.5% (APatK) - is the least stable. If batch_size*minibatches_per_epoch < 10k, it behaves as a uniform random variable.\n",
    "\n",
    "* Plotting metrics over training time may be a good way to analyze which architectures work better.\n",
    "\n",
    "* Once you are sure your network aint gonna crash, it's worth letting it train for a few hours of an average laptop's time to see it's true potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[101798 378172  64652 264312  25582 305816   5860 152496 272624 148828\\n 142816  68420 160599  14700  52290 302622 148812 312521 127945  43484\\n 136688 151934 190335 181716 322327 309687 288058 244389 245389 104980\\n 226665 262953  56742 255177  70258 200968  82477 235651 241770 406837\\n 147822 261691  81939 377795 153616 315044 398441 293894  38876 426624\\n 382964  69899 348669  30251 310210 336411  48452 381268 320314 145846\\n 100170 261586 414952  88733 192314 430004 216792 120656 361016 142959\\n 412967  67026 338910 219133 374412 195584 185181 209479 251029  55865\\n 359138 213674 414008   1033 405715 274128 279722 152445 392051 375338\\n  50244 363294 176177   9891  58126 409492 359920  36066 247145 171328] not in index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-5a8bc7f7e417>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mb_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n\u001b[0;32m---> 14\u001b[0;31m         iterate_minibatches(desc_tr,title_tr,nontext_tr,target_tr,batchsize=batch_size,shuffle=True)):\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mminibatches_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-1914d7cc2394>\u001b[0m in \u001b[0;36miterate_minibatches\u001b[0;34m(*arrays, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mexcerpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexcerpt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/fed/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2054\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2056\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2057\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fed/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2098\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2099\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2100\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fed/anaconda2/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1229\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s not in index'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobjarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '[101798 378172  64652 264312  25582 305816   5860 152496 272624 148828\\n 142816  68420 160599  14700  52290 302622 148812 312521 127945  43484\\n 136688 151934 190335 181716 322327 309687 288058 244389 245389 104980\\n 226665 262953  56742 255177  70258 200968  82477 235651 241770 406837\\n 147822 261691  81939 377795 153616 315044 398441 293894  38876 426624\\n 382964  69899 348669  30251 310210 336411  48452 381268 320314 145846\\n 100170 261586 414952  88733 192314 430004 216792 120656 361016 142959\\n 412967  67026 338910 219133 374412 195584 185181 209479 251029  55865\\n 359138 213674 414008   1033 405715 274128 279722 152445 392051 375338\\n  50244 363294 176177   9891  58126 409492 359920  36066 247145 171328] not in index'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 100\n",
    "minibatches_per_epoch = 100\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    #training\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    \n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_tr,title_tr,nontext_tr,target_tr,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch:break\n",
    "            \n",
    "        loss,pred_probas = train_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print \"Train:\"\n",
    "    print '\\tloss:',b_loss/b_c\n",
    "    print '\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.)\n",
    "    print '\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "    print '\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "    \n",
    "    #evaluation\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch: break\n",
    "        loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print \"Val:\"\n",
    "    print '\\tloss:',b_loss/b_c\n",
    "    print '\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.)\n",
    "    print '\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "    print '\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"If you are seeing this, it's time to backup your notebook. No, really, 'tis too easy to mess up everything without noticing. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation\n",
    "Evaluate network over the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[ 94272 109635 105273  98163  13257  39030   2722  75589  32892  97340\\n  29774  63013  46361  73944  40827  52172  36362  34461  88094   2436\\n  22683  24900  48894  18984  74638  28590  27042  79832  76575  88674\\n   7027  15498  59872  80189  52591  79228  86988 105506  24688  30494\\n  89987  60143  85679  29727  48945  80233  15026  45001  74461  29219\\n  45939  51904  13685  66241  73554   2247  13560   5557  92130  69928\\n 101616  74512 108391 105684  76621   9138 103966  62607  55287    807\\n  27471  46128  54652  17420 108652  81570  92938  96395  99431  18705\\n  41569  58699  83608  38058  34883  73421  72629  53334  47908  71597\\n  21972  61436  92618  67454  68725  48586  91183  11440 106374   6807] not in index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-805bf1b1edb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mb_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n\u001b[0;32m----> 7\u001b[0;31m     iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_probas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_desc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_title\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_cat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-1914d7cc2394>\u001b[0m in \u001b[0;36miterate_minibatches\u001b[0;34m(*arrays, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mexcerpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexcerpt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/fed/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2054\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2056\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2057\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fed/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2098\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2099\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2100\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fed/anaconda2/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1229\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s not in index'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobjarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '[ 94272 109635 105273  98163  13257  39030   2722  75589  32892  97340\\n  29774  63013  46361  73944  40827  52172  36362  34461  88094   2436\\n  22683  24900  48894  18984  74638  28590  27042  79832  76575  88674\\n   7027  15498  59872  80189  52591  79228  86988 105506  24688  30494\\n  89987  60143  85679  29727  48945  80233  15026  45001  74461  29219\\n  45939  51904  13685  66241  73554   2247  13560   5557  92130  69928\\n 101616  74512 108391 105684  76621   9138 103966  62607  55287    807\\n  27471  46128  54652  17420 108652  81570  92938  96395  99431  18705\\n  41569  58699  83608  38058  34883  73421  72629  53334  47908  71597\\n  21972  61436  92618  67454  68725  48586  91183  11440 106374   6807] not in index'"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "epoch_y_true = []\n",
    "epoch_y_pred = []\n",
    "\n",
    "b_c = b_loss = 0\n",
    "for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "    iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "    loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "\n",
    "    b_loss += loss\n",
    "    b_c +=1\n",
    "\n",
    "    epoch_y_true.append(b_y)\n",
    "    epoch_y_pred.append(pred_probas)\n",
    "\n",
    "\n",
    "epoch_y_true = np.concatenate(epoch_y_true)\n",
    "epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "\n",
    "final_accuracy = accuracy_score(epoch_y_true,epoch_y_pred>0)\n",
    "final_auc = roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "final_apatk = APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "\n",
    "print \"Scores:\"\n",
    "print '\\tloss:',b_loss/b_c\n",
    "print '\\tacc:',final_accuracy\n",
    "print '\\tauc:',final_auc\n",
    "print '\\tap@k:',final_apatk\n",
    "score(final_accuracy,final_auc,final_apatk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main task\n",
    "\n",
    "* https://goo.gl/forms/eJwIeAbjxzVuo6vn1\n",
    "* Feel like Le'Cun:\n",
    " * accuracy > 0.95\n",
    " * AUC > 0.97\n",
    " * Average Precision at (test sample size * 0.025) > 0.99\n",
    " * And perhaps even farther\n",
    "\n",
    "* Casual mode\n",
    " * accuracy > 0.90\n",
    " * AUC > 0.95\n",
    " * Average Precision at (test sample size * 0.025) > 0.92\n",
    "\n",
    "* Remember the training, Luke\n",
    " * Dropout, regularization\n",
    " * Mommentum, RMSprop, ada*\n",
    " * etc etc etc\n",
    " \n",
    " * If you have background in texts, there may be a way to improve tokenizer, add some lemmatization, etc etc.\n",
    " * In case you know how not to shoot yourself in the foot with RNNs, they too may be of some use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
